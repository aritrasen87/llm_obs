{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a7c67e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY')\n",
    "os.environ['TAVILY_API_KEY'] = os.getenv('TAVILY_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c854f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Langsmith params for observability\n",
    "os.environ['LANGSMITH_API_KEY'] = os.getenv('LANGSMITH_API_KEY')\n",
    "os.environ['LANGSMITH_PROJECT'] = 'LLM_OBS_YT'\n",
    "os.environ['LANGSMITH_TRACING']=\"true\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a765a376",
   "metadata": {},
   "source": [
    "### RAG Vector DB Population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "153f24b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/57/y3hsdcy135b1wr5gzcw3b3kr0000gn/T/ipykernel_11750/1780032375.py:12: LangChainDeprecationWarning: The class `HuggingFaceBgeEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceBgeEmbeddings(\n",
      "/Users/aritrasen/Documents/code/agents_observability/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Failed to send telemetry event ClientStartEvent: capture() takes 1 positional argument but 3 were given\n",
      "Failed to send telemetry event ClientCreateCollectionEvent: capture() takes 1 positional argument but 3 were given\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "loader = PyPDFLoader('sample_doc.pdf')\n",
    "docs = loader.load()\n",
    "\n",
    "###  BGE Embddings\n",
    "\n",
    "from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
    "\n",
    "model_name = \"BAAI/bge-base-en-v1.5\"\n",
    "model_kwargs = {'device': 'cpu'}\n",
    "encode_kwargs = {'normalize_embeddings': True} # set True to compute cosine similarity\n",
    "embeddings = HuggingFaceBgeEmbeddings(\n",
    "    model_name=model_name,\n",
    "    model_kwargs=model_kwargs,\n",
    "    encode_kwargs=encode_kwargs,\n",
    ")\n",
    "\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "### Creating Retriever using Vector DB\n",
    "db = Chroma.from_documents(docs, embeddings)\n",
    "retriever = db.as_retriever(search_kwargs={\"k\": 3})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef13d2e",
   "metadata": {},
   "source": [
    "### RAG with LangChain (LCEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c764724",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "46b03359",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(question: str):\n",
    "\n",
    "    template = \"\"\"Answer the question based only on the following context:\n",
    "        {context}\n",
    "\n",
    "        Question: {question}\n",
    "        \"\"\"\n",
    "    prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "    retrieval_chain = (\n",
    "        {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "        | prompt\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "        )\n",
    "    result = retrieval_chain.invoke(question)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b3b31c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Failed to send telemetry event CollectionQueryEvent: capture() takes 1 positional argument but 3 were given\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multi-head attention is a key component of the transformer architecture that enhances the model's ability to capture various patterns and relationships within input data. It operates by applying the scaled dot-product attention function in parallel across multiple attention heads. Each attention head processes the input using its own set of learnable weights to generate different representations of the input data.\n",
      "\n",
      "Mathematically, the attention outputs from each head are concatenated and transformed into a single matrix to form a comprehensive representation of the input. This approach allows the model to learn and integrate information from various segments of the input sequence effectively. By utilizing multiple heads, the multi-head attention mechanism enables the transformer to jointly attend to information from different perspectives, which results in improved network performance in tasks such as natural language processing. \n",
      "\n",
      "Overall, multi-head attention enhances the representation of input contexts by merging information derived from distinct features of the attention mechanism, facilitating better handling of both short-range and long-range dependencies in sequential data.\n"
     ]
    }
   ],
   "source": [
    "response = generate_response(\"Tell me about mutlihead attention in transformers\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab22b21",
   "metadata": {},
   "source": [
    "### LangGraph Agent with RAG + WebSearch (MultiAgent Supervisor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "af792b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Langsmith params for observability\n",
    "os.environ['LANGSMITH_API_KEY'] = os.getenv('LANGSMITH_API_KEY')\n",
    "os.environ['LANGSMITH_PROJECT'] = 'LLM_OBS_YT'\n",
    "os.environ['LANGSMITH_TRACING']=\"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "855aae01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.prebuilt import create_react_agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8db125c",
   "metadata": {},
   "source": [
    "### Tools Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3b0241d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/57/y3hsdcy135b1wr5gzcw3b3kr0000gn/T/ipykernel_11750/1911882425.py:3: LangChainDeprecationWarning: The class `TavilySearchResults` was deprecated in LangChain 0.3.25 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-tavily package and should be used instead. To use it run `pip install -U :class:`~langchain-tavily` and import as `from :class:`~langchain_tavily import TavilySearch``.\n",
      "  tavily_tool = TavilySearchResults(max_results=5)\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "\n",
    "tavily_tool = TavilySearchResults(max_results=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3701660a",
   "metadata": {},
   "source": [
    "### Create specialized Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "66376a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Research Agent for Web Search\n",
    "\n",
    "def web_search(query: str) -> str:\n",
    "    \"\"\"Search the web for information.\"\"\"\n",
    "    docs = tavily_tool.invoke({\"query\": query})\n",
    "    web_results = \"\\n\".join([d[\"content\"] for d in docs])\n",
    "    return web_results\n",
    "\n",
    "research_agent = create_react_agent(\n",
    "    model=llm,\n",
    "    tools=[web_search],\n",
    "    name=\"research_expert\",\n",
    "   prompt=\"You are a world class researcher with access to web search.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8670d209",
   "metadata": {},
   "outputs": [],
   "source": [
    "## RAG Agent\n",
    "\n",
    "def rag_search(query:str):\n",
    "    \"Function to do RAG search\"\n",
    "    docs = retriever.invoke(\n",
    "            query,\n",
    "        )\n",
    "    return \"\\nRetrieved documents:\\n\" + \"\".join(\n",
    "        [\n",
    "            f\"\\n\\n===== Document {str(i)} =====\\n\" + doc.page_content\n",
    "            for i, doc in enumerate(docs)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "rag_agent = create_react_agent(\n",
    "    model=llm,\n",
    "    tools=[rag_search],\n",
    "    name=\"rag_expert\",\n",
    "    prompt=\"You are a RAG tool with access to transformer applications on Deep Learning related tasks.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5fbd2710",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph_supervisor import create_supervisor\n",
    "\n",
    "workflow = create_supervisor(\n",
    "    agents=[research_agent, rag_agent],\n",
    "    model=llm,\n",
    "    prompt=(\n",
    "        \"You are a supervisor managing a web search expert and a RAG search expert. \"\n",
    "        \"For current events and information, use research_agent.\"\n",
    "        \"For transformer related information , use rag_agent.\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# Compile and run\n",
    "app = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5e60513d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Task supervisor with path ('__pregel_pull', 'supervisor') wrote to unknown channel is_last_step, ignoring it.\n",
      "Task supervisor with path ('__pregel_pull', 'supervisor') wrote to unknown channel remaining_steps, ignoring it.\n"
     ]
    }
   ],
   "source": [
    "result = app.invoke({\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Tell me about mutlihead attention in transformers\"\n",
    "        }\n",
    "    ]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ed740cd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Multi-head attention is a key concept in transformer architectures, allowing models to focus on different parts of the input sequence through multiple attention heads. Each head learns to pay attention to different aspects of the input, facilitating the capture of richer and more diverse information. It plays a crucial role in improving the performance of models in various natural language processing tasks. If you need further details or specific applications, feel free to ask!'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result['messages'][-1].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3771e9a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Task supervisor with path ('__pregel_pull', 'supervisor') wrote to unknown channel is_last_step, ignoring it.\n",
      "Task supervisor with path ('__pregel_pull', 'supervisor') wrote to unknown channel remaining_steps, ignoring it.\n"
     ]
    }
   ],
   "source": [
    "result = app.invoke(\n",
    "    {\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"who is the winner of Last T20 Cricket World Cup?\"\n",
    "        }\n",
    "    ]}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "08595e00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"India won the last T20 Cricket World Cup, held in 2024, defeating South Africa by 7 runs in the final. This victory marks India's second T20 World Cup title, following their win in 2007.\""
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result['messages'][-1].content "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf31ae8a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
